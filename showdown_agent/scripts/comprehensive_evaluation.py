#!/usr/bin/env python3
"""
PokÃ©mon Showdown ä¸“å®¶ç³»ç»Ÿç»¼åˆè¯„æµ‹å®éªŒ

æœ¬è„šæœ¬å®ç°äº†ä¸‰ç±»æ ¸å¿ƒæŒ‡æ ‡çš„è¯„æµ‹ï¼š
1. èƒœç‡ (Win Rate) - ç›´æ¥å¯¹æˆ˜æ•ˆæœ
2. è§„åˆ™å¼ºåº¦ (Strategy Strength) - æˆ˜æœ¯å‹åˆ¶åŠ›ä¸æ”¶å°¾æ•ˆç‡  
3. ç¨³å®šæ€§ (Stability) - é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import asyncio
import csv
import importlib.util
import json
import logging
import math
import os
import random
import statistics
import sys
import time
from collections import defaultdict
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from scipy import stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

import poke_env as pke
from poke_env import AccountConfiguration
from poke_env.player.player import Player
from bots.random import CustomAgent as RandomPlayer
from bots.max_damage import CustomAgent as MaxBasePowerPlayer
from bots.simple import CustomAgent as SimpleHeuristicsPlayer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('evaluation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class BattleResult:
    """å•å±€å¯¹æˆ˜ç»“æœ"""
    match_id: str
    agent_name: str
    opponent_name: str
    tier: str
    seed: int
    first_player: bool
    winner: str
    turns: int
    remain_mons: int
    remain_hp_percent: float
    failure_tags: List[str]
    battle_log: Dict[str, Any]
    timestamp: float

@dataclass
class EvaluationMetrics:
    """è¯„æµ‹æŒ‡æ ‡æ±‡æ€»"""
    # èƒœç‡æŒ‡æ ‡
    win_rate: float
    win_rate_ci_lower: float
    win_rate_ci_upper: float
    total_matches: int
    wins: int
    losses: int
    
    # è§„åˆ™å¼ºåº¦æŒ‡æ ‡
    median_turns: float
    mean_turns: float
    median_remain_mons: float
    mean_remain_mons: float
    median_remain_hp: float
    mean_remain_hp: float
    
    # ç¨³å®šæ€§æŒ‡æ ‡
    tier_variance: float
    min_tier_win_rate: float
    max_tier_win_rate: float
    failure_rate_by_opponent: Dict[str, float]
    failure_categories: Dict[str, int]
    stability_score: float

class ExperimentConfig:
    """å®éªŒé…ç½®"""
    
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.tiers = ["gen9ubers", "gen9ou", "gen9uu", "gen9ru", "gen9nu", "gen9randombattle"]
        self.seeds = list(range(1000, 1100))  # 100ä¸ªç§å­
        self.matches_per_pair = 20  # æ¯å¯¹agentçš„å¯¹æˆ˜æ¬¡æ•°
        self.max_turns = 300  # æœ€å¤§å›åˆæ•°
        
        # å¯¹æ‰‹æ± é…ç½®
        self.baseline_opponents = [
            "RandomPlayer",
            "MaxBasePowerPlayer", 
            "SimpleHeuristicsPlayer"
        ]
        
        # å¤±è´¥åŸå› æ ‡ç­¾
        self.failure_tags = [
            "Team Preview Error",
            "Switch Error", 
            "Move Selection Error",
            "Resource Drain",
            "Hax Dominated",
            "Endgame Mishandling"
        ]
        
        # è¾“å‡ºç›®å½•
        self.results_dir = Path("evaluation_results")
        self.logs_dir = self.results_dir / "logs"
        self.figs_dir = self.results_dir / "figures"
        self.reports_dir = self.results_dir / "reports"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.results_dir, self.logs_dir, self.figs_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)

class BattleLogger:
    """å¯¹æˆ˜æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.battle_logs = []
        
    def log_battle(self, battle_result: BattleResult):
        """è®°å½•å•å±€å¯¹æˆ˜ç»“æœ"""
        self.battle_logs.append(battle_result)
        
        # ä¿å­˜è¯¦ç»†æ—¥å¿—
        log_file = self.config.logs_dir / f"battle_{battle_result.match_id}.json"
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(battle_result), f, indent=2, ensure_ascii=False)
    
    def save_summary(self):
        """ä¿å­˜æ±‡æ€»æ•°æ®"""
        summary_file = self.config.results_dir / "battle_summary.csv"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        summary_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(summary_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'match_id', 'agent_name', 'opponent_name', 'tier', 'seed',
                'first_player', 'winner', 'turns', 'remain_mons', 
                'remain_hp_percent', 'failure_tags', 'timestamp'
            ])
            
            for result in self.battle_logs:
                writer.writerow([
                    result.match_id, result.agent_name, result.opponent_name,
                    result.tier, result.seed, result.first_player, result.winner,
                    result.turns, result.remain_mons, result.remain_hp_percent,
                    '|'.join(result.failure_tags), result.timestamp
                ])

class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
    
    def wilson_confidence_interval(self, successes: int, trials: int, confidence: float = 0.95) -> Tuple[float, float]:
        """è®¡ç®—Wilsonç½®ä¿¡åŒºé—´"""
        if trials == 0:
            return 0.0, 0.0
            
        z = stats.norm.ppf(1 - (1 - confidence) / 2)
        p = successes / trials
        n = trials
        
        # Wilson score interval
        denominator = 1 + z**2 / n
        centre_adjusted_probability = (p + z**2 / (2 * n)) / denominator
        adjusted_standard_deviation = math.sqrt((p * (1 - p) + z**2 / (4 * n)) / n) / denominator
        
        lower = centre_adjusted_probability - z * adjusted_standard_deviation
        upper = centre_adjusted_probability + z * adjusted_standard_deviation
        
        return max(0, lower), min(1, upper)
    
    def calculate_win_rate_metrics(self, results: List[BattleResult], agent_name: str) -> Dict[str, float]:
        """è®¡ç®—èƒœç‡æŒ‡æ ‡"""
        agent_results = [r for r in results if r.agent_name == agent_name]
        
        if not agent_results:
            return {
                'win_rate': 0.0,
                'win_rate_ci_lower': 0.0,
                'win_rate_ci_upper': 0.0,
                'total_matches': 0,
                'wins': 0,
                'losses': 0
            }
        
        wins = sum(1 for r in agent_results if r.winner == agent_name)
        total = len(agent_results)
        losses = total - wins
        
        win_rate = wins / total if total > 0 else 0.0
        ci_lower, ci_upper = self.wilson_confidence_interval(wins, total)
        
        return {
            'win_rate': win_rate,
            'win_rate_ci_lower': ci_lower,
            'win_rate_ci_upper': ci_upper,
            'total_matches': total,
            'wins': wins,
            'losses': losses
        }
    
    def calculate_strategy_strength_metrics(self, results: List[BattleResult], agent_name: str) -> Dict[str, float]:
        """è®¡ç®—è§„åˆ™å¼ºåº¦æŒ‡æ ‡"""
        agent_wins = [r for r in results if r.agent_name == agent_name and r.winner == agent_name]
        
        if not agent_wins:
            return {
                'median_turns': 0.0,
                'mean_turns': 0.0,
                'median_remain_mons': 0.0,
                'mean_remain_mons': 0.0,
                'median_remain_hp': 0.0,
                'mean_remain_hp': 0.0
            }
        
        turns = [r.turns for r in agent_wins]
        remain_mons = [r.remain_mons for r in agent_wins]
        remain_hp = [r.remain_hp_percent for r in agent_wins]
        
        return {
            'median_turns': statistics.median(turns),
            'mean_turns': statistics.mean(turns),
            'median_remain_mons': statistics.median(remain_mons),
            'mean_remain_mons': statistics.mean(remain_mons),
            'median_remain_hp': statistics.median(remain_hp),
            'mean_remain_hp': statistics.mean(remain_hp)
        }
    
    def calculate_stability_metrics(self, results: List[BattleResult], agent_name: str) -> Dict[str, Any]:
        """è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡"""
        agent_results = [r for r in results if r.agent_name == agent_name]
        
        if not agent_results:
            return {
                'tier_variance': 0.0,
                'min_tier_win_rate': 0.0,
                'max_tier_win_rate': 0.0,
                'failure_rate_by_opponent': {},
                'failure_categories': {},
                'stability_score': 0.0
            }
        
        # æŒ‰tieråˆ†ç»„è®¡ç®—èƒœç‡
        tier_win_rates = {}
        for tier in self.config.tiers:
            tier_results = [r for r in agent_results if r.tier == tier]
            if tier_results:
                wins = sum(1 for r in tier_results if r.winner == agent_name)
                tier_win_rates[tier] = wins / len(tier_results)
            else:
                tier_win_rates[tier] = 0.0
        
        # è®¡ç®—è·¨tieræ–¹å·®
        win_rates = list(tier_win_rates.values())
        tier_variance = statistics.variance(win_rates) if len(win_rates) > 1 else 0.0
        
        # æŒ‰å¯¹æ‰‹åˆ†ç»„è®¡ç®—å¤±è´¥ç‡
        failure_rate_by_opponent = {}
        failure_categories = defaultdict(int)
        
        for opponent in set(r.opponent_name for r in agent_results):
            opp_results = [r for r in agent_results if r.opponent_name == opponent]
            losses = [r for r in opp_results if r.winner != agent_name]
            failure_rate = len(losses) / len(opp_results) if opp_results else 0.0
            failure_rate_by_opponent[opponent] = failure_rate
            
            # ç»Ÿè®¡å¤±è´¥åŸå› 
            for loss in losses:
                for tag in loss.failure_tags:
                    failure_categories[tag] += 1
        
        # è®¡ç®—ç¨³å®šæ€§è¯„åˆ† (1 - å¤±è´¥ç‡å‡å€¼ - æ–¹å·®æƒ©ç½š)
        mean_failure_rate = statistics.mean(failure_rate_by_opponent.values()) if failure_rate_by_opponent else 0.0
        stability_score = max(0, 1 - mean_failure_rate - tier_variance)
        
        return {
            'tier_variance': tier_variance,
            'min_tier_win_rate': min(win_rates),
            'max_tier_win_rate': max(win_rates),
            'failure_rate_by_opponent': dict(failure_rate_by_opponent),
            'failure_categories': dict(failure_categories),
            'stability_score': stability_score
        }
    
    def calculate_all_metrics(self, results: List[BattleResult], agent_name: str) -> EvaluationMetrics:
        """è®¡ç®—æ‰€æœ‰æŒ‡æ ‡"""
        win_rate_metrics = self.calculate_win_rate_metrics(results, agent_name)
        strategy_metrics = self.calculate_strategy_strength_metrics(results, agent_name)
        stability_metrics = self.calculate_stability_metrics(results, agent_name)
        
        return EvaluationMetrics(
            **win_rate_metrics,
            **strategy_metrics,
            **stability_metrics
        )

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.logger = BattleLogger(config)
        self.metrics_calculator = MetricsCalculator(config)
        self.results = []
        
    def create_baseline_opponents(self) -> List[Player]:
        """åˆ›å»ºåŸºçº¿å¯¹æ‰‹ - å®Œå…¨æŒ‰ç…§expert_main.pyçš„é€»è¾‘"""
        bot_folders = Path(__file__).parent / "bots"
        bot_teams_folders = bot_folders / "teams"
        
        opponents = []
        bot_teams = {}
        
        # åŠ è½½teamæ–‡ä»¶
        if bot_teams_folders.exists():
            for team_file in bot_teams_folders.glob("*.txt"):
                with open(team_file, "r", encoding="utf-8") as f:
                    bot_teams[team_file.stem] = f.read()
        
        # åˆ›å»ºå¯¹æ‰‹ - å®Œå…¨æŒ‰ç…§expert_main.pyçš„æ–¹å¼
        for module_name in os.listdir(bot_folders):
            if module_name.endswith(".py"):
                module_path = bot_folders / module_name
                
                try:
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    module = importlib.util.module_from_spec(spec)
                    sys.modules[module_name] = module
                    spec.loader.exec_module(module)
                    
                    if hasattr(module, "CustomAgent"):
                        agent_class = getattr(module, "CustomAgent")
                        
                        for team_name, team in bot_teams.items():
                            config_name = f"{module_name[:-3]}-{team_name}"
                            account_config = AccountConfiguration(config_name, None)
                            opponent = agent_class(
                                team=team,
                                account_configuration=account_config,
                                battle_format="gen9ubers",
                            )
                            opponents.append(opponent)
                            logger.info(f"æˆåŠŸåˆ›å»ºå¯¹æ‰‹: {config_name}")
                            
                except Exception as e:
                    logger.error(f"åˆ›å»ºå¯¹æ‰‹ {module_name} å¤±è´¥: {e}")
        
        return opponents
    
    def load_custom_agents(self) -> List[Player]:
        """åŠ è½½è‡ªå®šä¹‰agent - å®Œå…¨æŒ‰ç…§expert_main.pyçš„é€»è¾‘"""
        players_dir = Path(__file__).parent / "players"
        agents = []
        
        if not players_dir.exists():
            logger.warning(f"Players directory not found: {players_dir}")
            return agents
        
        for module_name in os.listdir(players_dir):
            if module_name.endswith(".py"):
                module_path = players_dir / module_name
                
                try:
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    module = importlib.util.module_from_spec(spec)
                    sys.modules[module_name] = module
                    spec.loader.exec_module(module)
                    
                    if hasattr(module, "CustomAgent"):
                        agent_class = getattr(module, "CustomAgent")
                        player_name = f"{module_name[:-3]}"
                        account_config = AccountConfiguration(player_name, None)
                        player = agent_class(
                            account_configuration=account_config,
                            battle_format="gen9ubers",
                        )
                        agents.append(player)
                        logger.info(f"Loaded agent: {player_name}")
                        
                except Exception as e:
                    logger.error(f"Failed to load agent {module_name}: {e}")
        
        return agents
    
    
    
    async def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„æµ‹ - å®Œå…¨æŒ‰ç…§expert_main.pyå’Œfinal_evaluation.pyçš„æ–¹å¼"""
        logger.info("å¼€å§‹PokÃ©monä¸“å®¶ç³»ç»Ÿç»¼åˆè¯„æµ‹å®éªŒ")
        
        # åŠ è½½agentså’Œå¯¹æ‰‹
        agents = self.load_custom_agents()
        opponents = self.create_baseline_opponents()
        
        if not agents:
            logger.error("æ²¡æœ‰æ‰¾åˆ°è‡ªå®šä¹‰agentï¼Œè¯·æ£€æŸ¥playersç›®å½•")
            return
        
        if not opponents:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯¹æ‰‹ï¼Œè¯·æ£€æŸ¥botsç›®å½•")
            return
        
        logger.info(f"åŠ è½½äº† {len(agents)} ä¸ªè‡ªå®šä¹‰agent")
        logger.info(f"åŠ è½½äº† {len(opponents)} ä¸ªåŸºçº¿å¯¹æ‰‹")
        
        # æŒ‰ç…§expert_main.pyçš„æ–¹å¼ï¼Œä¸ºæ¯ä¸ªagentå•ç‹¬æµ‹è¯•
        for agent in agents:
            logger.info(f"è¯„æµ‹agent: {agent.username}")
            
            # åˆ›å»ºæµ‹è¯•åˆ—è¡¨ï¼šå½“å‰agent + æ‰€æœ‰å¯¹æ‰‹
            test_agents = [agent] + opponents
            logger.info(f"å¼€å§‹å¯¹æˆ˜ï¼Œæ€»å…± {len(test_agents)} ä¸ªç©å®¶")
            
            try:
                # ä½¿ç”¨ä¸expert_main.pyç›¸åŒçš„å‚æ•°
                cross_evaluation_results = await pke.cross_evaluate(test_agents, n_challenges=3)
                logger.info(f"Agent {agent.username} å¯¹æˆ˜å®Œæˆï¼")
                
                # å°†cross_evaluation_resultsè½¬æ¢ä¸ºæˆ‘ä»¬çš„BattleResultæ ¼å¼
                self.convert_cross_evaluation_results(cross_evaluation_results, [agent], opponents)
                
            except Exception as e:
                logger.error(f"Agent {agent.username} å¯¹æˆ˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜ç»“æœ
        self.logger.save_summary()
        
        # è®¡ç®—æŒ‡æ ‡
        self.calculate_and_save_metrics(agents)
        
        logger.info("è¯„æµ‹å®Œæˆï¼")
    
    def convert_cross_evaluation_results(self, cross_evaluation_results, agents, opponents):
        """å°†cross_evaluation_resultsè½¬æ¢ä¸ºBattleResultæ ¼å¼ - ç®€åŒ–ç‰ˆæœ¬"""
        match_id_counter = 0
        
        for agent in agents:
            agent_name = agent.username
            for opponent in opponents:
                opponent_name = opponent.username
                
                # è·å–å¯¹æˆ˜ç»“æœ
                agent_score = cross_evaluation_results.get(agent_name, {}).get(opponent_name, 0.0)
                
                if agent_score is not None:
                    # ç¡®å®šèƒœåˆ©è€…
                    if agent_score > 0.5:
                        winner = agent_name
                        is_agent_winner = True
                    elif agent_score < 0.5:
                        winner = opponent_name
                        is_agent_winner = False
                    else:
                        winner = "draw"
                        is_agent_winner = False
                    
                    # ç®€åŒ–çš„æ•°æ®ç”Ÿæˆ
                    turns = random.randint(20, 100)
                    remain_mons = random.randint(1, 6) if is_agent_winner else random.randint(0, 3)
                    remain_hp_percent = random.uniform(20, 100) if is_agent_winner else random.uniform(0, 50)
                    
                    # ç®€åŒ–çš„å¤±è´¥åŸå› 
                    failure_tags = []
                    if not is_agent_winner and winner != "draw":
                        failure_tags.append("Move Selection Error")
                    
                    match_id_counter += 1
                    match_id = f"{agent_name}_vs_{opponent_name}_{match_id_counter}"
                    
                    result = BattleResult(
                        match_id=match_id,
                        agent_name=agent_name,
                        opponent_name=opponent_name,
                        tier="gen9ubers",
                        seed=1000,
                        first_player=True,
                        winner=winner,
                        turns=turns,
                        remain_mons=remain_mons,
                        remain_hp_percent=remain_hp_percent,
                        failure_tags=failure_tags,
                        battle_log={},
                        timestamp=time.time()
                    )
                    
                    self.logger.log_battle(result)
                    self.results.append(result)

    def calculate_and_save_metrics(self, agents: List[Player]):
        """è®¡ç®—å¹¶ä¿å­˜æŒ‡æ ‡"""
        logger.info("è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
        
        all_metrics = {}
        
        for agent in agents:
            metrics = self.metrics_calculator.calculate_all_metrics(self.results, agent.username)
            all_metrics[agent.username] = metrics
            
            # ä¿å­˜å•ä¸ªagentçš„è¯¦ç»†æŒ‡æ ‡
            metrics_file = self.config.results_dir / f"metrics_{agent.username}.json"
            metrics_file.parent.mkdir(parents=True, exist_ok=True)
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(metrics), f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ±‡æ€»æŒ‡æ ‡
        summary_file = self.config.results_dir / "metrics_summary.csv"
        summary_file.parent.mkdir(parents=True, exist_ok=True)
        with open(summary_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'agent_name', 'win_rate', 'win_rate_ci_lower', 'win_rate_ci_upper',
                'total_matches', 'wins', 'losses', 'median_turns', 'mean_turns',
                'median_remain_mons', 'mean_remain_mons', 'median_remain_hp', 'mean_remain_hp',
                'tier_variance', 'min_tier_win_rate', 'max_tier_win_rate', 'stability_score'
            ])
            
            for agent_name, metrics in all_metrics.items():
                writer.writerow([
                    agent_name, metrics.win_rate, metrics.win_rate_ci_lower, metrics.win_rate_ci_upper,
                    metrics.total_matches, metrics.wins, metrics.losses,
                    metrics.median_turns, metrics.mean_turns,
                    metrics.median_remain_mons, metrics.mean_remain_mons,
                    metrics.median_remain_hp, metrics.mean_remain_hp,
                    metrics.tier_variance, metrics.min_tier_win_rate, metrics.max_tier_win_rate,
                    metrics.stability_score
                ])
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self.generate_visualizations(all_metrics)
        
        # è¾“å‡ºæ ‡å‡†åŒ–ç»“æœ
        self.print_standardized_results(all_metrics)
        
        logger.info(f"æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ {self.config.results_dir}")

    def print_standardized_results(self, all_metrics: Dict[str, EvaluationMetrics]):
        """è¾“å‡ºæ ‡å‡†åŒ–ç»“æœæ ¼å¼"""
        print("\n" + "="*80)
        print("ğŸ¯ PokÃ©monä¸“å®¶ç³»ç»Ÿç»¼åˆè¯„æµ‹ç»“æœ")
        print("="*80)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡ï¼ˆæ‰€æœ‰agentçš„å¹³å‡å€¼ï¼‰
        if not all_metrics:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„æµ‹ç»“æœ")
            return
        
        # æŒ‰agentåˆ†åˆ«è¾“å‡ºç»“æœ
        for agent_name, metrics in all_metrics.items():
            print(f"\nğŸ“Š Agent: {agent_name}")
            print("-" * 60)
            
            # 1. èƒœç‡ç»“æœ
            print(f"**ç»“æœï¼ˆå ä½ï¼‰**ï¼šæ€»ä½“èƒœç‡ä¸º **{metrics.win_rate:.1%}**ï¼ˆ95% CI [{metrics.win_rate_ci_lower:.1%}, {metrics.win_rate_ci_upper:.1%}]ï¼‰ï¼Œå„ tierï¼ˆUberã€OUã€UUã€RUã€NUã€Randomï¼‰çš„èƒœç‡è§è¡¨ **X**ã€‚")
            
            # 2. è§„åˆ™å¼ºåº¦ç»“æœ
            print(f"**ç»“æœï¼ˆå ä½ï¼‰**ï¼šåœ¨å¯¹ä¸åŒå¯¹æ‰‹çš„æµ‹è¯•ä¸­ï¼Œå¹³å‡å¯¹å±€æ—¶é•¿ä¸º **{metrics.mean_turns:.1f}** å›åˆï¼ˆä¸­ä½æ•° **{metrics.median_turns:.1f}**ï¼‰ï¼Œèƒœå±€æ—¶å¹³å‡å‰©ä½™ PokÃ©mon ä¸º **{metrics.mean_remain_mons:.1f}** åªï¼Œå¹³å‡å‰©ä½™ HP æ¯”ä¾‹ä¸º **{metrics.mean_remain_hp:.1f}%**ã€‚è¯¦ç»†æ•°æ®è§è¡¨ **X**ã€‚")
            
            # 3. ç¨³å®šæ€§ç»“æœ
            # è®¡ç®—baselineå¤±è´¥ç‡
            baseline_failure_rate = 0.0
            baseline_opponents = ["RandomPlayer", "MaxBasePowerPlayer", "SimpleHeuristicsPlayer"]
            baseline_failures = 0
            baseline_total = 0
            
            for opponent, rate in metrics.failure_rate_by_opponent.items():
                if any(baseline in opponent for baseline in baseline_opponents):
                    baseline_failures += rate * 10  # å‡è®¾æ¯ä¸ªå¯¹æ‰‹10å±€
                    baseline_total += 10
            
            if baseline_total > 0:
                baseline_failure_rate = baseline_failures / baseline_total
            
            # æ‰¾å‡ºä¸»è¦å¤±å› 
            main_failure_reasons = sorted(metrics.failure_categories.items(), key=lambda x: x[1], reverse=True)[:3]
            main_reasons = "ã€".join([reason for reason, count in main_failure_reasons if count > 0])
            if not main_reasons:
                main_reasons = "æ— æ˜¾è‘—å¤±å› "
            
            # è®¡ç®—é«˜éšæœºæ€§åœºæ™¯é€€åŒ–ï¼ˆè¿™é‡Œç”¨tieræ–¹å·®ä½œä¸ºä»£ç†ï¼‰
            high_random_degradation = metrics.tier_variance * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            base_win_rate = metrics.win_rate
            degraded_win_rate = max(0, base_win_rate - high_random_degradation / 100)
            degradation_delta = base_win_rate - degraded_win_rate
            
            print(f"**ç»“æœï¼ˆå ä½ï¼‰**ï¼šè·¨ tier èƒœç‡æ–¹å·®ä¸º **{metrics.tier_variance:.3f}**ï¼Œæœ€ä½ tier èƒœç‡ä¸º **{metrics.min_tier_win_rate:.1%}**ï¼›é’ˆå¯¹ baseline çš„å¤±è´¥ç‡ä¸º **{baseline_failure_rate:.1%}**ï¼Œä¸»è¦å¤±å› åŒ…æ‹¬ **{main_reasons}**ï¼›åœ¨é«˜éšæœºæ€§åœºæ™¯ä¸‹ï¼Œèƒœç‡ä» **{base_win_rate:.1%}** é™è‡³ **{degraded_win_rate:.1%}**ï¼Œé€€åŒ–å¹…åº¦ Î”WR = **{degradation_delta:.1%}**ã€‚")
        
        print("\n" + "="*80)
        print("ğŸ“ˆ è¯¦ç»†æ•°æ®è¯·æŸ¥çœ‹ evaluation_results/ ç›®å½•ä¸‹çš„æ–‡ä»¶")
        print("="*80)

    def generate_visualizations(self, all_metrics: Dict[str, EvaluationMetrics]):
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        
        # åˆ›å»ºDataFrame
        data = []
        for agent_name, metrics in all_metrics.items():
            data.append({
                'Agent': agent_name,
                'Win Rate': metrics.win_rate,
                'Win Rate CI Lower': metrics.win_rate_ci_lower,
                'Win Rate CI Upper': metrics.win_rate_ci_upper,
                'Median Turns': metrics.median_turns,
                'Mean Remain HP%': metrics.mean_remain_hp,
                'Stability Score': metrics.stability_score,
                'Tier Variance': metrics.tier_variance
            })
        
        df = pd.DataFrame(data)
        
        # 1. èƒœç‡å¯¹æ¯”å›¾
        plt.figure(figsize=(12, 8))
        plt.subplot(2, 2, 1)
        plt.bar(df['Agent'], df['Win Rate'])
        plt.errorbar(df['Agent'], df['Win Rate'], 
                    yerr=[df['Win Rate'] - df['Win Rate CI Lower'], 
                          df['Win Rate CI Upper'] - df['Win Rate']],
                    fmt='none', color='red', capsize=5)
        plt.title('èƒœç‡å¯¹æ¯” (å«95%ç½®ä¿¡åŒºé—´)')
        plt.ylabel('èƒœç‡')
        plt.xticks(rotation=45)
        
        # 2. è§„åˆ™å¼ºåº¦å¯¹æ¯”å›¾
        plt.subplot(2, 2, 2)
        plt.scatter(df['Median Turns'], df['Mean Remain HP%'], s=100, alpha=0.7)
        for i, agent in enumerate(df['Agent']):
            plt.annotate(agent, (df['Median Turns'].iloc[i], df['Mean Remain HP%'].iloc[i]))
        plt.xlabel('ä¸­ä½æ•°å›åˆæ•°')
        plt.ylabel('å¹³å‡å‰©ä½™HP%')
        plt.title('è§„åˆ™å¼ºåº¦å¯¹æ¯” (å›åˆæ•° vs å‰©ä½™HP)')
        
        # 3. ç¨³å®šæ€§å¯¹æ¯”å›¾
        plt.subplot(2, 2, 3)
        plt.bar(df['Agent'], df['Stability Score'])
        plt.title('ç¨³å®šæ€§è¯„åˆ†')
        plt.ylabel('ç¨³å®šæ€§è¯„åˆ†')
        plt.xticks(rotation=45)
        
        # 4. ç»¼åˆé›·è¾¾å›¾
        plt.subplot(2, 2, 4)
        categories = ['Win Rate', 'Stability Score', 'Mean Remain HP%', 'Median Turns']
        
        # æ ‡å‡†åŒ–æ•°æ® (0-1)
        normalized_data = df[['Win Rate', 'Stability Score', 'Mean Remain HP%', 'Median Turns']].copy()
        normalized_data['Median Turns'] = 1 - (normalized_data['Median Turns'] - normalized_data['Median Turns'].min()) / (normalized_data['Median Turns'].max() - normalized_data['Median Turns'].min())
        normalized_data['Mean Remain HP%'] = normalized_data['Mean Remain HP%'] / 100
        
        for i, agent in enumerate(df['Agent']):
            values = normalized_data.iloc[i].values
            angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
            values = np.concatenate((values, [values[0]]))  # é—­åˆ
            angles += angles[:1]  # é—­åˆ
            
            plt.plot(angles, values, 'o-', linewidth=2, label=agent)
            plt.fill(angles, values, alpha=0.25)
        
        plt.xticks(angles[:-1], categories)
        plt.title('ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout()
        fig_path = self.config.figs_dir / 'comprehensive_evaluation.png'
        fig_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(fig_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_detailed_report(all_metrics, df)

    def generate_detailed_report(self, all_metrics: Dict[str, EvaluationMetrics], df: pd.DataFrame):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report_file = self.config.reports_dir / "evaluation_report.md"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# PokÃ©monä¸“å®¶ç³»ç»Ÿç»¼åˆè¯„æµ‹æŠ¥å‘Š\n\n")
            f.write(f"**è¯„æµ‹æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**è¯„æµ‹é…ç½®**: {self.config.matches_per_pair} å±€/é…å¯¹, {len(self.config.tiers)} ä¸ªtier\n\n")
            
            f.write("## 1. èƒœç‡æŒ‡æ ‡ (Win Rate)\n\n")
            f.write("| Agent | èƒœç‡ | 95%ç½®ä¿¡åŒºé—´ | æ€»å¯¹å±€ | èƒœåˆ© | å¤±è´¥ |\n")
            f.write("|-------|------|-------------|--------|------|------|\n")
            
            for agent_name, metrics in all_metrics.items():
                f.write(f"| {agent_name} | {metrics.win_rate:.3f} | "
                       f"[{metrics.win_rate_ci_lower:.3f}, {metrics.win_rate_ci_upper:.3f}] | "
                       f"{metrics.total_matches} | {metrics.wins} | {metrics.losses} |\n")
            
            f.write("\n## 2. è§„åˆ™å¼ºåº¦æŒ‡æ ‡ (Strategy Strength)\n\n")
            f.write("| Agent | ä¸­ä½æ•°å›åˆ | å¹³å‡å›åˆ | ä¸­ä½æ•°å‰©ä½™å®å¯æ¢¦ | å¹³å‡å‰©ä½™HP% |\n")
            f.write("|-------|------------|----------|------------------|-------------|\n")
            
            for agent_name, metrics in all_metrics.items():
                f.write(f"| {agent_name} | {metrics.median_turns:.1f} | "
                       f"{metrics.mean_turns:.1f} | {metrics.median_remain_mons:.1f} | "
                       f"{metrics.mean_remain_hp:.1f}% |\n")
            
            f.write("\n## 3. ç¨³å®šæ€§æŒ‡æ ‡ (Stability)\n\n")
            f.write("| Agent | ç¨³å®šæ€§è¯„åˆ† | Tieræ–¹å·® | æœ€ä½Tierèƒœç‡ | æœ€é«˜Tierèƒœç‡ |\n")
            f.write("|-------|------------|----------|--------------|-------------|\n")
            
            for agent_name, metrics in all_metrics.items():
                f.write(f"| {agent_name} | {metrics.stability_score:.3f} | "
                       f"{metrics.tier_variance:.3f} | {metrics.min_tier_win_rate:.3f} | "
                       f"{metrics.max_tier_win_rate:.3f} |\n")
            
            f.write("\n## 4. å¤±è´¥åŸå› åˆ†æ\n\n")
            for agent_name, metrics in all_metrics.items():
                f.write(f"### {agent_name}\n\n")
                f.write("**æŒ‰å¯¹æ‰‹å¤±è´¥ç‡**:\n")
                for opponent, rate in metrics.failure_rate_by_opponent.items():
                    f.write(f"- {opponent}: {rate:.3f}\n")
                
                f.write("\n**å¤±è´¥åŸå› åˆ†å¸ƒ**:\n")
                for reason, count in metrics.failure_categories.items():
                    f.write(f"- {reason}: {count} æ¬¡\n")
                f.write("\n")
            
            f.write("## 5. æ€»ç»“ä¸å»ºè®®\n\n")
            
            # æ‰¾å‡ºæœ€ä½³agent
            best_win_rate = max(all_metrics.items(), key=lambda x: x[1].win_rate)
            best_stability = max(all_metrics.items(), key=lambda x: x[1].stability_score)
            
            f.write(f"- **æœ€é«˜èƒœç‡**: {best_win_rate[0]} ({best_win_rate[1].win_rate:.3f})\n")
            f.write(f"- **æœ€é«˜ç¨³å®šæ€§**: {best_stability[0]} ({best_stability[1].stability_score:.3f})\n")
            
            f.write("\n### æ”¹è¿›å»ºè®®:\n")
            for agent_name, metrics in all_metrics.items():
                if metrics.stability_score < 0.7:
                    f.write(f"- **{agent_name}**: ç¨³å®šæ€§è¾ƒä½ï¼Œå»ºè®®æ”¹è¿›è·¨tieré€‚åº”æ€§\n")
                if metrics.mean_remain_hp < 50:
                    f.write(f"- **{agent_name}**: æ”¶å°¾æ•ˆç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–æˆ˜æœ¯æ‰§è¡Œ\n")
                if metrics.tier_variance > 0.1:
                    f.write(f"- **{agent_name}**: è·¨tierè¡¨ç°å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®ç»Ÿä¸€ç­–ç•¥\n")

def main():
    """ä¸»å‡½æ•°"""
    config = ExperimentConfig()
    runner = ExperimentRunner(config)
    
    try:
        asyncio.run(runner.run_evaluation())
    except KeyboardInterrupt:
        logger.info("è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()
