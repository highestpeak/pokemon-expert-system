#!/usr/bin/env python3
"""
PokÃ©monä¸“å®¶ç³»ç»Ÿè¯„æµ‹è¿è¡Œè„šæœ¬

æä¾›ç®€å•çš„å‘½ä»¤è¡Œæ¥å£æ¥è¿è¡Œä¸åŒç±»å‹çš„è¯„æµ‹å®éªŒ
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from comprehensive_evaluation import ExperimentRunner, ExperimentConfig
from experiment_config import CONFIG_MANAGER, QUICK_CONFIG, DEFAULT_CONFIG, COMPREHENSIVE_CONFIG, STABILITY_CONFIG

def run_quick_evaluation():
    """è¿è¡Œå¿«é€Ÿè¯„æµ‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    print("ğŸš€ å¯åŠ¨å¿«é€Ÿè¯„æµ‹...")
    print(f"é…ç½®: {QUICK_CONFIG.matches_per_pair} å±€/é…å¯¹, {len(QUICK_CONFIG.enabled_tiers)} ä¸ªtier")
    
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    config = ExperimentConfig()
    config.matches_per_pair = QUICK_CONFIG.matches_per_pair
    config.tiers = QUICK_CONFIG.enabled_tiers
    config.seeds = list(range(1000, 1000 + QUICK_CONFIG.seeds_count))
    config.max_turns = QUICK_CONFIG.max_turns
    config.results_dir = Path(QUICK_CONFIG.results_dir)
    
    runner = ExperimentRunner(config)
    asyncio.run(runner.run_evaluation())

def run_default_evaluation():
    """è¿è¡Œé»˜è®¤è¯„æµ‹"""
    print("ğŸš€ å¯åŠ¨é»˜è®¤è¯„æµ‹...")
    print(f"é…ç½®: {DEFAULT_CONFIG.matches_per_pair} å±€/é…å¯¹, {len(DEFAULT_CONFIG.enabled_tiers)} ä¸ªtier")
    
    config = ExperimentConfig()
    config.matches_per_pair = DEFAULT_CONFIG.matches_per_pair
    config.tiers = DEFAULT_CONFIG.enabled_tiers
    config.seeds = list(range(1000, 1000 + DEFAULT_CONFIG.seeds_count))
    config.max_turns = DEFAULT_CONFIG.max_turns
    config.results_dir = Path(DEFAULT_CONFIG.results_dir)
    
    runner = ExperimentRunner(config)
    asyncio.run(runner.run_evaluation())

def run_comprehensive_evaluation():
    """è¿è¡Œå…¨é¢è¯„æµ‹"""
    print("ğŸš€ å¯åŠ¨å…¨é¢è¯„æµ‹...")
    print(f"é…ç½®: {COMPREHENSIVE_CONFIG.matches_per_pair} å±€/é…å¯¹, {len(COMPREHENSIVE_CONFIG.enabled_tiers)} ä¸ªtier")
    print("âš ï¸  æ³¨æ„ï¼šå…¨é¢è¯„æµ‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
    
    config = ExperimentConfig()
    config.matches_per_pair = COMPREHENSIVE_CONFIG.matches_per_pair
    config.tiers = COMPREHENSIVE_CONFIG.enabled_tiers
    config.seeds = list(range(1000, 1000 + COMPREHENSIVE_CONFIG.seeds_count))
    config.max_turns = COMPREHENSIVE_CONFIG.max_turns
    config.results_dir = Path(COMPREHENSIVE_CONFIG.results_dir)
    
    runner = ExperimentRunner(config)
    asyncio.run(runner.run_evaluation())

def run_stability_test():
    """è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
    print("ğŸš€ å¯åŠ¨ç¨³å®šæ€§æµ‹è¯•...")
    print(f"é…ç½®: {STABILITY_CONFIG.matches_per_pair} å±€/é…å¯¹, ä¸“æ³¨äºç¨³å®šæ€§æŒ‡æ ‡")
    
    config = ExperimentConfig()
    config.matches_per_pair = STABILITY_CONFIG.matches_per_pair
    config.tiers = STABILITY_CONFIG.enabled_tiers
    config.seeds = list(range(1000, 1000 + STABILITY_CONFIG.seeds_count))
    config.max_turns = STABILITY_CONFIG.max_turns
    config.results_dir = Path(STABILITY_CONFIG.results_dir)
    
    runner = ExperimentRunner(config)
    asyncio.run(runner.run_evaluation())

def run_custom_evaluation(config_file: str):
    """è¿è¡Œè‡ªå®šä¹‰é…ç½®è¯„æµ‹"""
    print(f"ğŸš€ å¯åŠ¨è‡ªå®šä¹‰è¯„æµ‹: {config_file}")
    
    try:
        custom_config = CONFIG_MANAGER.load_config(config_file)
        print(f"é…ç½®: {custom_config.matches_per_pair} å±€/é…å¯¹, {len(custom_config.enabled_tiers)} ä¸ªtier")
        
        # éªŒè¯é…ç½®
        errors = CONFIG_MANAGER.validate_config(custom_config)
        if errors:
            print("âŒ é…ç½®é”™è¯¯:")
            for error in errors:
                print(f"  - {error}")
            return
        
        config = ExperimentConfig()
        config.matches_per_pair = custom_config.matches_per_pair
        config.tiers = custom_config.enabled_tiers
        config.seeds = list(range(1000, 1000 + custom_config.seeds_count))
        config.max_turns = custom_config.max_turns
        config.results_dir = Path(custom_config.results_dir)
        
        runner = ExperimentRunner(config)
        asyncio.run(runner.run_evaluation())
        
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

def create_custom_config():
    """äº¤äº’å¼åˆ›å»ºè‡ªå®šä¹‰é…ç½®"""
    print("ğŸ”§ åˆ›å»ºè‡ªå®šä¹‰è¯„æµ‹é…ç½®")
    
    experiment_name = input("å®éªŒåç§°: ").strip() or "Custom Evaluation"
    matches_per_pair = int(input("æ¯å¯¹å¯¹æˆ˜æ¬¡æ•° (é»˜è®¤20): ") or "20")
    
    print("\nå¯ç”¨çš„Tier:")
    for i, (tier, info) in enumerate(CONFIG_MANAGER.tiers.items(), 1):
        print(f"{i}. {tier} - {info.description}")
    
    tier_input = input("é€‰æ‹©tier (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,2,3): ").strip()
    if tier_input:
        tier_indices = [int(x.strip()) - 1 for x in tier_input.split(",")]
        enabled_tiers = [list(CONFIG_MANAGER.tiers.keys())[i] for i in tier_indices]
    else:
        enabled_tiers = ["gen9ou", "gen9randombattle"]
    
    print("\nå¯ç”¨çš„å¯¹æ‰‹:")
    for i, (opponent, info) in enumerate(CONFIG_MANAGER.opponents.items(), 1):
        print(f"{i}. {opponent} - {info.description}")
    
    opponent_input = input("é€‰æ‹©å¯¹æ‰‹ (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,2,3): ").strip()
    if opponent_input:
        opponent_indices = [int(x.strip()) - 1 for x in opponent_input.split(",")]
        enabled_opponents = [list(CONFIG_MANAGER.opponents.keys())[i] for i in opponent_indices]
    else:
        enabled_opponents = ["RandomPlayer", "MaxBasePowerPlayer"]
    
    # åˆ›å»ºé…ç½®
    custom_config = CONFIG_MANAGER.create_custom_config(
        experiment_name=experiment_name,
        matches_per_pair=matches_per_pair,
        enabled_tiers=enabled_tiers,
        enabled_opponents=enabled_opponents
    )
    
    # ä¿å­˜é…ç½®
    config_file = f"{experiment_name.lower().replace(' ', '_')}_config.json"
    CONFIG_MANAGER.save_config(custom_config, config_file)
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    print(f"è¿è¡Œå‘½ä»¤: python run_evaluation.py --custom {config_file}")

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("""
ğŸ® PokÃ©monä¸“å®¶ç³»ç»Ÿè¯„æµ‹å·¥å…·

ä½¿ç”¨æ–¹æ³•:
  python run_evaluation.py [é€‰é¡¹]

é€‰é¡¹:
  --quick              å¿«é€Ÿè¯„æµ‹ (5å±€/é…å¯¹, ç”¨äºæµ‹è¯•)
  --default            é»˜è®¤è¯„æµ‹ (20å±€/é…å¯¹, æ¨è)
  --comprehensive      å…¨é¢è¯„æµ‹ (50å±€/é…å¯¹, æ‰€æœ‰tier)
  --stability          ç¨³å®šæ€§æµ‹è¯• (100å±€/é…å¯¹, ä¸“æ³¨ç¨³å®šæ€§)
  --custom <æ–‡ä»¶>      è‡ªå®šä¹‰é…ç½®è¯„æµ‹
  --create-config      äº¤äº’å¼åˆ›å»ºè‡ªå®šä¹‰é…ç½®
  --help               æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  python run_evaluation.py --quick                    # å¿«é€Ÿæµ‹è¯•
  python run_evaluation.py --default                  # æ ‡å‡†è¯„æµ‹
  python run_evaluation.py --create-config            # åˆ›å»ºè‡ªå®šä¹‰é…ç½®
  python run_evaluation.py --custom my_config.json    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®

è¯„æµ‹æŒ‡æ ‡:
  1. èƒœç‡ (Win Rate) - ç›´æ¥å¯¹æˆ˜æ•ˆæœ
  2. è§„åˆ™å¼ºåº¦ (Strategy Strength) - æˆ˜æœ¯å‹åˆ¶åŠ›ä¸æ”¶å°¾æ•ˆç‡
  3. ç¨³å®šæ€§ (Stability) - é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›

è¾“å‡ºæ–‡ä»¶:
  - evaluation_results/metrics_summary.csv     # æŒ‡æ ‡æ±‡æ€»
  - evaluation_results/figures/                # å¯è§†åŒ–å›¾è¡¨
  - evaluation_results/reports/                # è¯¦ç»†æŠ¥å‘Š
  - evaluation_results/logs/                   # å¯¹æˆ˜æ—¥å¿—
""")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PokÃ©monä¸“å®¶ç³»ç»Ÿè¯„æµ‹å·¥å…·")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿè¯„æµ‹")
    parser.add_argument("--default", action="store_true", help="é»˜è®¤è¯„æµ‹")
    parser.add_argument("--comprehensive", action="store_true", help="å…¨é¢è¯„æµ‹")
    parser.add_argument("--stability", action="store_true", help="ç¨³å®šæ€§æµ‹è¯•")
    parser.add_argument("--custom", type=str, help="è‡ªå®šä¹‰é…ç½®æ–‡ä»¶")
    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºè‡ªå®šä¹‰é…ç½®")
    parser.add_argument("--help-detailed", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©")
    
    args = parser.parse_args()
    
    if args.help_detailed:
        show_help()
        return
    
    if args.create_config:
        create_custom_config()
        return
    
    if args.quick:
        run_quick_evaluation()
    elif args.default:
        run_default_evaluation()
    elif args.comprehensive:
        run_comprehensive_evaluation()
    elif args.stability:
        run_stability_test()
    elif args.custom:
        run_custom_evaluation(args.custom)
    else:
        print("è¯·é€‰æ‹©è¯„æµ‹ç±»å‹ï¼Œä½¿ç”¨ --help æŸ¥çœ‹é€‰é¡¹")
        print("æ¨èä½¿ç”¨: python run_evaluation.py --default")

if __name__ == "__main__":
    main()
